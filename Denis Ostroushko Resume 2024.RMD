---
title: "Resume"
author: ""
output:
  pagedown::html_resume:
    link-external-newwindow: true
    css: 
      - ex_test.css         # OVERIDE CERTAIN FUNCTIONS OF CSS
      - resume               # DEFAULT FILE            
    self_contained: true
    number_sections: no
  html_document:
    df_print: paged
    toc: yes
  knit: pagedown::chrome_print
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE}

#     remotes::install_github("mitchelloharawild/icons")

#Load the good stuff
library(tidyverse)
library(pagedown)
library(icons)
library(knitr)
library(ggthemes)
library(fontawesome)

#Token

#Working directory for .RMD
knitr::opts_knit$set(echo = TRUE,
                     root.dir = rprojroot::find_rstudio_root_file())

#Controlling figure output in markdown
knitr::opts_chunk$set(
#  fig.height =   
  fig.width = 6,
#  fig.asp = .5,
#  out.width = "90%",
#  out.height = 
 fig.align = "center",
  cache = FALSE
)

#Set Theme for ggplot2
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom"))

#Set Scientific notation output for knitr
options(scipen = 999)

knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "!H", fig.height=4, fig.width=7, fig.align='center')
options(scipen=999)


# older more saturated color: #A2E4B8

sidebar_color <- '#dcf7e5' # make sure that this color mathces 'sidebar-background-color' in the CSS file
font_color <- '#36454f'

```

Aside
====================================

```{r create a plot for top corner, echo = FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(igraph)
library(wesanderson)

set.seed(123)  # set the seed for reproducibility

# Create the graph with 30 nodes
g <- sample_gnp(30, 0.5)

# Get the 'Royal2' color palette
colors <- wes_palette("BottleRocket2")

# Assign a color to each node randomly
V(g)$color <- colors[sample(1:length(colors), vcount(g), replace = TRUE)]

# Assign a size to each node randomly
V(g)$size <- runif(vcount(g), 10, 30)

png("./myplot.png",  width=300, height=300)
    
    # Plot the graph
    par(bg = sidebar_color , mar=c(0,0,0,0)+.01)
    plot(g, vertex.color = V(g)$color, 
         vertex.size = V(g)$size * .75, 
         vertex.label = NA,
         edge.color = adjustcolor("black", alpha = .5), 
         edge.size = 1000, 
         edge.width = 2, 
         bg = sidebar_color)

dev.off()

```

```{r echo = FALSE, out.width = "100%", eval=F}
include_graphics("./myplot.png") 
```

Contact Info{#contact}
------------------------------------  

<i class="fa fa-phone-alt"></i> +1-612-528-1163 <br>  <br> 
<i class="fa fa-tablet-alt"></i> [denisostroushko1@gmail.com](mailto:denisostroushko1@gmail.com) <br>  <br> 
<i class="fa fa-linkedin-square"></i> [LinkedIn Profile](https://www.linkedin.com/in/denis-ostroushko/) <br>  <br> 
<i class="fa fa-sync"></i> [denisostroushko1.github.io](https://denisostroushko1.github.io) <br>  <br> 
<i class="fa fa-github"></i> [github.com/denisostroushko1](https://github.com/denisostroushko1)

Technical Skills 
------------------------------------  

```{r}

build_skill_bars <- function(skill_data, section_title) {
  skill_data %>%   # take in the data that says what software I use and what rating I give it
    filter(section == section_title) %>% # seelct only tech skills 
                                          # for now this is an irrelevant filter since all I included were tech skills  
    
    ggplot(aes(x= reorder(skill, level), y = 5)) + # start makjng rhe plot 
    
      geom_col(fill = sidebar_color) + # use columns, which will turn into the cards 
    
      geom_col(aes(x= reorder(skill, level), y = level), fill = "black") + 
     
      coord_flip() + # turn vertical bars into horizontal bars 
    
      geom_text(aes(label = skill,  y = 0.25), hjust = 0, size = 12, color = 'white') + # text that overlays software name over the bar 
    
      expand_limits(y = c(0,max(skill_data[skill_data$section == section_title,]$level))) + # make it so that the best skill is the biggest bar that occupies the entere width
    
      labs(x = NULL,
           y = NULL) + # no names or anything, we only want the bars 
      
      theme_void() + # the most minimal theme possible 
      
      theme(
        panel.background = element_rect(fill = sidebar_color, colour = NA), # fill the background with color of choice and remove the balck outline 
        plot.background  = element_rect(fill = sidebar_color, colour = NA) # same here 
      ) 
}

```


```{r}

# some person make an example from Nick's resume; will ustilize this example to crete skill bars 
# https://www.mattleary.com/blog/2020-01-04-pagedown_resume/

resume_data <- 
  data.frame(
    rbind(
      c("Tech", "R/Rstudio", 5),
      c("Tech", "SAS/SQL", 4),
      c("Tech", "Git", 3),
      c("Tech", "Python", 2),
      c("Tech", "AWS S3", 2)
    )
  ) # this data set is simply the 'data' for the bars that will show my skills
    # actual score does not matter, as far as I udnerstand from testing differnet values 
    # what matters is the relevant score of each given skill to the max score defined in the data 

colnames(resume_data) <- c("section", "skill", "level")

resume_data$level <- as.numeric(resume_data$level)

# use function from a previous plot to create a ggplot that actually creates a 'plot' under the picture 
build_skill_bars(
  skill_data = resume_data, 
  section_title = "Tech"
  )

```

Analytical Skills
------------------------------------

Frequentist and Bayesian Statistics  <br><br>
Predictive and Inferential Modeling  <br><br>
Correlated and Survival Outcomes  <br><br>
Observational Data Causal Inference  <br><br>
Study Design, Cohort Construction  <br><br>
Simulation Studies 

  
Data Science Skills
------------------------------------


Static Reports, Interactive ShinyApps   <br> <br> 
`ggplot` and `plotly` Visualizations   <br> <br>
Pipelines from Source Data to Final Products  <br> <br>
Big EHR, Claims, Insurance Data Processing   <br> <br>
Strong RX, CPT, Diagnosis Codes Understanding <br> <br>


$~~~$  
*Document powered by* `pagedown`
Source code in [Git](https://github.com/denisostroushko1/pagedown_resume) 

Last updated on `r format(Sys.Date(), "%m/%d/%Y")`

Main
====================================

Denis Ostroushko {#title}
------------------------------------

<!-- if a resume needs a header than include it here --> 

Education {data-icon=graduation-cap}
------------------------------------

### University of Minnesota School of Public Health  
<!--bullet point  is above --> 
**MS Biostatistics**  <!-- first entry, major and degree in bold. THIS IS A Subtitle--> 

Minneapolis, MN <!-- second position is always a location --> 
 
2024 - 2022  <!-- dates have to be listed as Newer - Older in the pagedown format --> 

<!-- This is the body of a bullet point pretty much --> 
<!-- we can leave this open --> 
### University of Minnesota - Morris 

**BA Mathematics, Statistics** *(Double Major)*

Morris, MN 

2019 - 2015 

Research Experience {data-icon=pencil} 
------------------------------------

<!-- title  thingy --> 


### Graduate Research Assistant

University of Minnesota, Division of Biostatistics 

Minneapolis, MN 

May 2024 - August 2023 

*Master's Thesis*

- Conducted an observational study aimed at optimizing the definition of Mild Cognitive Impairment (MCI) and Alzheimer's Disease (AD) diagnoses using Fairview Electronic Health Records Data.
- Conducted literature review and data exploration to develop precise definitions based on established inclusion/exclusion criteria and insights from data observations.
- Proposed a sample optimized for inferential study purposes, laid the groundwork to study association between non-pharmaceutical interventions and MCI-to-AD progression

*Data Integration Predictive Methods for AD Identification*

- Implemented regression and machine learning methods to develop a pipeline identifying variables most associated with AD presence from multiple -omics data sets. 
- Used SIDA, a novel data-integration method, for multi-view feature selection & inference in Alzheimerâ€™s biomarker detection. 
- Showed that use of multiple -omics data sets improves AUC by 10-20% when compared with individual view approaches. 


Industry Experience {data-icon=suitcase} 
------------------------------------

### Medica

*Healthcare Analyst II*

Minneapolis, MN 

August 2023 - June 2021 <!-- Date new to date old --> 

- Led a study to assess a member assistance program. Developed a Cox model to understand average treatment effect. Showed that $1,000,000 annualized savings was a statistically significant figure. Used subgroup analysis to propose further improvement steps.  
- Contributed to improvement of risk predictions for hospital readmission. Constructed data from claims in enrollment using SAS. Developed LASSO logistic regression and random forest models in R. Contributed to an improved model, increasing accuracy and AUC by 17% over previous production model. 
- Designed a difference-in-differences study with propensity score matching for causal analysis of treatment intervention. Used bootstrap methods to get variance of estimators. Performed sample size and power calculations to suggest next steps for program improvement.  

\pagebreak

### Medica

*Healthcare Analyst I*

Minneapolis, MN 

June 2021 - June 2019 <!-- Date new to date old --> 

- Automated actuarial completion factors model using SAS/SQL. Improved estimation granularity, reduced prediction error by 35%. Reduced delivery time for the final product from several business days to three hours. 
- Consulted internal teams on ad-hoc analyses, inclusion/exclusion criteria, and inference from small samples of data to promote further pursuit of initiatives.
-  Developed analysis ready data sets from claims and enrollment data, distributed data sets to collaborators. Extracted analysis
features from ICD, CPT, DRG, RX codes. Automated text processing using SAS and R software to extract additional
information.
- Led the introduction of benchmarks from Milliman proprietary software. Improved identification of cost savings opportunities and increased strategy transparency.
- Routinely performed medical cost and utilization trend analysis. Contributed to high-level business oriented presentations with insightful summary statements, origins of observations, and data visualizations created in R using ggplot. 

Personal Projects {data-icon=computer} 
------------------------------------

### Interactive Analysis of Soccer Data 

N/A

N/A

Present - April 2023 

- Continuously working on my [interactive app](https://kexite.shinyapps.io/fb_ref_dashboard/) to summarize player-level match data from [FBRef.com](https://fbref.com/en/). Automated daily data update using [github actions](https://github.com/denisostroushko1/soccer_dashboard) and storage of data in AWS S3. 
- Developed interactive UI using `ShinyApp` functionality in R.
- Created summary reports using aggregated statistics, ranked data, and interactive `plotly` data visualization. 
- Used high-dimensional aggregate season-level data  to study player similarity. Adapted and developed multiple versions of similarity metrics based on euclidean and weighted distance between data points in high dimensional space. 
- Working on Bayesian hierarchical level models to study distributions of observed player statistics. Aiming to use these studies to develop detailed scouting reports. 

### Interactive Analysis of Financial Markets

N/A

N/A

Present - April 2021 

- Developed a [dashboard](https://kexite.shinyapps.io/01_Deployment/) to summarize and visualize time-series financial data from cryptocurrency markers. 
- Developed interactive UI using `ShinyApp` functionality in R. Automated data update using [github actions](https://github.com/denisostroushko1/crypto_data_update), api tools, and AWS S3 data storage. Code for the app is not public yet. 
- Implemented analysis of moving averages, RSI, volatility patterns, and other common financial market analysis tools to gauge insights from historical data. 
- Developed a version of a risk detection model for each asset type. Risk model summarizes current market data and volatility patterns to identify periods of severe over- and under-valuations of cryptocurrency assets. 